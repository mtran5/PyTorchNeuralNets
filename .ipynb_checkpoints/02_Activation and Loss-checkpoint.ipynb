{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement various different activation functions and loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the ```ReLU``` activation function and its backward gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13648\\3210357052.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, X):\n",
    "        # TODO: Implement the forward pass\n",
    "        self.mask = X < 0\n",
    "        X[self.mask] = 0\n",
    "        return X\n",
    "    def backward(self, loss_grad):\n",
    "        # TODO: Implement the backward pass\n",
    "        self.grad = loss_grad @ self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct implementation!\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "y = relu(y)\n",
    "\n",
    "relu_torch = torch.nn.ReLU()\n",
    "y_ts = relu_torch(y_ts)\n",
    "\n",
    "assert utils.check_norm_diff(y, y_ts.detach().numpy())\n",
    "print(\"Correct implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sigmoid\n",
    "Implement the `sigmoid` activation function and its backward gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __call__(self, X):\n",
    "        # TODO: implement the forward pass\n",
    "        self.sigmoid = 1/(1 + np.exp(X)) \n",
    "        return self.sigmoid\n",
    "    def backward(self, loss_grad):\n",
    "        # TODO: implement the backward pass\n",
    "        return loss_grad @ self.sigmoid @ (1 - self.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Mean Squared Error\n",
    "\n",
    "Implement the Mean Squared Error (MSE) loss function and its backward gradient. The result should be a single scalar value that is the sum of all elements divided by the number of elements in the output. It is common to multiply this loss by 0.5 so that the backward pass has the unity term.\n",
    "\n",
    "Resources:\n",
    "\n",
    "* [MSELoss PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss():\n",
    "    def __call__(true, pred):\n",
    "        #TODO: Implement the forward pass\n",
    "        self.loss = np.mean((true - pred)**2, keepdims=False)\n",
    "        self.pred = pred\n",
    "        self.true = true\n",
    "        return self.loss\n",
    "    def backward(self):\n",
    "        self.grad = np.mean(self.pred - self.true)\n",
    "        return self.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
