{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advance-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitted-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"./solutions\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-frank",
   "metadata": {},
   "source": [
    "In this notebook, we will implement the linear layer, which is one of the most basic block of many neural network. We will also implement back-propagation on linear layer, which is how they can learn. The linear classifier simply multiply an input matrix $X$ with a trainable weight matrix $W$ to get an output layer. To make the linear classifier output change value, it is neccessary to also add a bias term $b$. Our goal for the entire module is to learn the XOR function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-ambassador",
   "metadata": {},
   "source": [
    "### Warmup Exercise: Matmul\n",
    "Implement a naive algorithm to perform matrix multiplication. You cannot use any matrix multiplication library for this task. You are given two matrices $A \\in \\mathbb{R}^{m \\times n} $ and $B \\in \\mathbb{R}^{n \\times k}$ as inputs. \n",
    "\n",
    "Constraints:\n",
    "* $1 \\leq m, n, o \\leq 10$\n",
    "* Elements of $A$ and $B$ are between -10 and 10\n",
    "* It is guaranteed that $A$ and $B$ dimensions matched\n",
    "\n",
    "Recall matrix multiplication using the following link:\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "superb-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def naive_matmul(A: List[List], B: List[List]) -> List[List]:\n",
    "    # Your task is to implement this function\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mental-niger",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-520889372892>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_norm_diff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\MINH\\Python\\NNFromScratch\\solutions\\utils.py\u001b[0m in \u001b[0;36mcheck_norm_diff\u001b[1;34m(true, pred, tolerance)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcheck_norm_diff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(4, 5)\n",
    "B = np.random.randn(5, 7)\n",
    "C = naive_matmul(A.tolist(), B.tolist())\n",
    "utils.check_norm_diff(C, np.matmul(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-child",
   "metadata": {},
   "source": [
    "Onward, we will use NumPy and PyTorch, which are Python libraries with excellent matrix manipulation capabilities. They are Python libraries built on top of C++ backbones, which make them much faster than our naive implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "correct-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using naive Python: 0.0006s\n",
      "Using Numpy: 0.1342s\n",
      "Reduction: 21654.22%\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import statistics\n",
    "SETUP_CODE = \"\"\"\n",
    "import solutions_01\n",
    "from __main__ import naive_matmul\n",
    "import numpy as np\n",
    "import utils\n",
    "A, B = utils.generate_random_matrices()\n",
    "\"\"\"\n",
    "TEST_CODE_PYTHON = \"\"\"\n",
    "naive_matmul(A, B)\n",
    "\"\"\"\n",
    "python_time = timeit.repeat(setup=SETUP_CODE,\n",
    "             stmt=TEST_CODE_PYTHON,\n",
    "             repeat=100,\n",
    "             number=100)\n",
    "print(f\"Using naive Python: {sum(python_time):.4f}s\")\n",
    "\n",
    "TEST_CODE_NUMPY = \"\"\"\n",
    "np.matmul(A, B)\n",
    "\"\"\"\n",
    "numpy_time = timeit.repeat(setup=SETUP_CODE,\n",
    "             stmt=TEST_CODE_NUMPY,\n",
    "             repeat=100,\n",
    "             number=100)\n",
    "print(f\"Using Numpy: {sum(numpy_time):.4f}s\")\n",
    "print(f\"Reduction: {sum(numpy_time) * 100 / sum(python_time):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-place",
   "metadata": {},
   "source": [
    "### Exercise 1: Linear Layer, forward pass\n",
    "\n",
    "Implement the forward pass of a linear layer that accepts two parameters: ```input_size``` and ```output_size```\n",
    "\n",
    "Constraints:\n",
    "* input $X$ is of size $m \\times n$ where $n$ is the number of features and $m$ is the number of samples\n",
    "* output $Y$ is of size $m \\times k$ where $k$ is the number of features in the output\n",
    "* $0 < m, n \\leq 10$\n",
    "* $0 < k \\leq 1024$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wired-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.stdev = 1e-4\n",
    "        self.w = np.random.randn(input_size, output_size) * self.stdev\n",
    "        self.b = np.zeros((output_size, ))\n",
    "        \n",
    "        self.w_grads = None\n",
    "        self.b_grads = None\n",
    "        self.X_grads = None\n",
    "        \n",
    "    def __call__(self, X: np.array) -> np.array:\n",
    "        # TODO: Implement the forward call\n",
    "        # You can use NumPy for this task\n",
    "        return None\n",
    "    \n",
    "    def backward(self, loss_grads: np.array) -> None:\n",
    "        # TODO: implement the backward call to change the gradients in relation to the loss\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "several-appeal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct implementation!\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass\n",
    "in_features = 4\n",
    "out_features = 6\n",
    "num_samples = 2\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(num_samples, in_features) * 1000\n",
    "fc = Linear(in_features, out_features)\n",
    "y = fc(X)\n",
    "y_expected = [[-0.07984137,  0.07732081, -0.09612012, -0.16418351, -0.10232742,  0.21378361],\n",
    "             [ 0.19450549, -0.00636151, -0.00086576, -0.24383612, -0.1165521,  0.23753465]]\n",
    "\n",
    "assert utils.check_norm_diff(y, y_expected, tolerance=1e-7)\n",
    "print(\"Correct implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-patch",
   "metadata": {},
   "source": [
    "### Exercise 2: Backpropagation\n",
    "\n",
    "Implement ```Linear.backward()``` function, which will compute the loss gradients with respect to the weights $W$, the input $X$, and the bias $b$.\n",
    "\n",
    "\n",
    "Resources:\n",
    "* [Computing Neural Network Gradients](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiWrP_j2NH5AhUMHDQIHdYUBXkQFnoECA8QAQ&url=https%3A%2F%2Fweb.stanford.edu%2Fclass%2Fcs224n%2Freadings%2Fgradient-notes.pdf&usg=AOvVaw2tREdV0bcqhk8A9L9Xwkqj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "presidential-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grads = np.ones((num_samples, out_features))\n",
    "fc.backward(loss_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-trust",
   "metadata": {},
   "source": [
    "To assert that our calculation is correct, we will compare the gradient we got with PyTorch ```backward()``` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "incorporate-filing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct implementation!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X_ts = torch.tensor(X, requires_grad=True).to(torch.double)\n",
    "X_ts.retain_grad()\n",
    "w_ts = torch.tensor(fc.w, requires_grad=True).to(torch.double)\n",
    "w_ts.retain_grad()\n",
    "b_ts = torch.tensor(fc.b, requires_grad=True).to(torch.double)\n",
    "b_ts.retain_grad()\n",
    "y_ts = torch.matmul(X_ts, w_ts) + b_ts\n",
    "\n",
    "y_ts.backward(gradient=torch.ones(y_ts.shape))\n",
    "\n",
    "import utils\n",
    "assert(utils.check_norm_diff(w_ts.grad.numpy(), fc.w_grads))\n",
    "assert(utils.check_norm_diff(b_ts.grad.numpy(), fc.b_grads))\n",
    "assert(utils.check_norm_diff(X_ts.grad.numpy(), fc.X_grads))\n",
    "print(\"Correct implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-dutch",
   "metadata": {},
   "source": [
    "### Exercise 3: Neural Network\n",
    "\n",
    "Given a two-layer neural network that has the following properties:\n",
    "* Layer 1: A Linear layer with 2 input features and 4 output features.\n",
    "* Layer 2: A Linear layer with 4 input features and 1 output features.\n",
    "\n",
    "Implement the forward and the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "discrete-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayersNet():\n",
    "    def __init__(self, in_features: int=2, num_hidden: int=4, out_features: int=1):\n",
    "        self.linear1 = Linear(in_features, num_hidden)\n",
    "        self.linear2 = Linear(num_hidden, out_features)\n",
    "\n",
    "    def __call__(self, X: np.array) -> np.array:\n",
    "        #TODO: Implement the forward call\n",
    "        return None\n",
    "\n",
    "    def backward(self, loss_grads: np.array) -> None:\n",
    "        # TODO: Implement the backward call\n",
    "        pass\n",
    "\n",
    "    def update_weight(self, optimizer: Callable):\n",
    "        # Apply gradient descent on the weights\n",
    "        optimizer(self.linear1.w, self.linear1.w_grads)\n",
    "        optimizer(self.linear1.b, self.linear1.b_grads)\n",
    "        optimizer(self.linear2.w, self.linear2.w_grads)\n",
    "        optimizer(self.linear2.b, self.linear2.b_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "organized-cream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct implementation!\n"
     ]
    }
   ],
   "source": [
    "in_features = 2\n",
    "num_hidden = 4\n",
    "out_features = 1\n",
    "num_samples = 4\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(num_samples, in_features) * 1e4\n",
    "net = TwoLayersNet(in_features, num_hidden, out_features)\n",
    "\n",
    "Y = net(X)\n",
    "Y_expected = [[ 6.47261839e-05], [ 3.42295178e-04], [-7.58308977e-05], [ 3.88537545e-04]]\n",
    "assert utils.check_norm_diff(Y, Y_expected)\n",
    "print(\"Correct implementation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "framed-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct implementation!\n"
     ]
    }
   ],
   "source": [
    "net.backward(np.ones(Y.shape))\n",
    "w1 = [[-2.52140472,  0.78230683, -2.26049155, -3.51587658],\n",
    "     [-1.94267423,  0.60274628, -1.74164768, -2.70888794]]\n",
    "b1 = [-0.00040513,  0.0001257,  -0.00036321, -0.00056492]\n",
    "w2 = [[-0.70463975], [-2.31910999], [-4.46216243], [-2.23791967]]\n",
    "b2 = [4.]\n",
    "\n",
    "assert utils.check_norm_diff(w1, net.linear1.w_grads)\n",
    "assert utils.check_norm_diff(b1, net.linear1.b_grads)\n",
    "assert utils.check_norm_diff(w2, net.linear2.w_grads)\n",
    "assert utils.check_norm_diff(b2, net.linear2.b_grads)\n",
    "print(\"Correct implementation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
